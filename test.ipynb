{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ideal-surrey",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.datasets.base'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aa566d950b1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.datasets.base'"
     ]
    }
   ],
   "source": [
    "# encoding:utf-8\n",
    "# Auther:Zhouqi\n",
    "# 17/3/6\n",
    "import sys\n",
    "import os\n",
    "from sklearn.datasets.base import Bunch\n",
    "import jieba\n",
    "import pickle\n",
    "import numpy\n",
    "import chardet\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "'''\n",
    "复旦大学的数据集，总共有9804篇文本，分为20个类别\n",
    "注意：TfidfVectorizer和CountVectorizer设置参数时，一定要设置好max_feature参数，不然会出现占用内存空间过大，溢出的情况！\n",
    "'''\n",
    "\n",
    "'''\n",
    "#########################\n",
    "语料库的文件目录：\n",
    "corpus目录\n",
    "      类别A\n",
    "        ----文件1.txt\n",
    "        ----文件2.txt\n",
    "      类别B\n",
    "        ----文件3.txt\n",
    "        ----文件4.txt\n",
    "#########################\n",
    "'''\n",
    "# 配置utf-8的输出环境\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "# 语料库的预处理类\n",
    "class Textprocess:\n",
    "    data_set = Bunch(target_name=[], label=[], filenames=[], content=[])\n",
    "    # 定义的原始语料库的词袋对象:data_set\n",
    "    # Bunch类提供一种key,value的对象形式\n",
    "    # target_name:所有分类集名称列表\n",
    "    # label:每个文件的分类标签列表\n",
    "    # filenames:文件名称\n",
    "    # contents:文件内容\n",
    "    vocabulary_count_bag = Bunch(\n",
    "        target_name=[], label=[], filenames=[], vcm=[], vcm_sum=[], vocabulary={})\n",
    "    # 语料库的词汇统计矩阵对象\n",
    "    # vcm 是词频统计矩阵\n",
    "    # vocabulary 是语料库的词汇列表\n",
    "    word_weight_bag = Bunch(target_name=[], label=[],\n",
    "                            filenames=[], tdm=[], vocabulary={})\n",
    "    # tdm是语料库权重矩阵对象\n",
    "    #构造方法\n",
    "    def __init__(self):\n",
    "        #语料库的原始路径\n",
    "        self.corpus_path = \"\"\n",
    "        #预处理后的语料库路径\n",
    "        self.pos_path = \"\"\n",
    "        #分词后的语料库路径\n",
    "        self.segment_path =\"\"\n",
    "        #词袋模型的语料库路径，也就是word_weight_bag，vocabulary_count_bag的存储路径\n",
    "        self.wordbag_path =\"\"\n",
    "        #停用词路径\n",
    "        self.stopword_path =\"\"\n",
    "        #训练集和测试集的set存储的文件名\n",
    "        self.trainset_name =\"\"\n",
    "        #词包存储的文件名\n",
    "        self.word_weight_bag_name =\"\"\n",
    "        self.vocabulary_count_bag_name =\"\"\n",
    "\n",
    "    #对语料库进行预处理，删除语料库的换行符，并持久化\n",
    "    #处理后在pos_path下建立与corpus_path相同的子目录和文件结构\n",
    "    def preprocess(self):\n",
    "        if (self.corpus_path ==\"\" or self.pos_path ==\"\"):\n",
    "            print(\"corpus_path or pos_path can not be empty!\")\n",
    "            return \n",
    "        #获取原始语料库目录下的所有文件\n",
    "        dir_list = os.listdir(self.corpus_path)\n",
    "        for mydir in dir_list:\n",
    "            #拼出分类子目录的路径\n",
    "            class_path =self.corpus_path +mydir+\"/\"\n",
    "            #获得子目录下的所有文件\n",
    "            file_list =os.listdir(class_path)\n",
    "            for file_path in file_list:\n",
    "                #得到文件的全名\n",
    "                file_name = class_path +file_path\n",
    "                file_open =open(file_name,'rb')\n",
    "                file_read =file_open.read()\n",
    "                #对读取的文本，进行按行切分字符串的处理，得到一个数组\n",
    "                corpus_array = file_read.splitlines()\n",
    "                file_read = \"\"\n",
    "                for line in corpus_array:\n",
    "                    #去掉每行两端的空格\n",
    "                    line = line.strip()\n",
    "                    #匹配中文字符\n",
    "                    #line = self.match_chinese(line)\n",
    "                    #print line\n",
    "                    #file_read = self.simple_pruneline(line,file_read)\n",
    "                    file_read =self.custom_pruneline(line,file_read)\n",
    "                #拼出分词后语料库的分类目录\n",
    "                pos_dir = self.pos_path +mydir +\"/\"\n",
    "                if not os.path.exists(pos_dir):\n",
    "                    os.makedirs(pos_dir)\n",
    "                file_write = open(pos_dir+file_path,'wb')\n",
    "                file_write.write(file_read)\n",
    "                file_write.close()\n",
    "                file_open.close()\n",
    "        print(\"语料库的预处理完成！\")\n",
    "        print(\"#######################################\")\n",
    "\n",
    "\n",
    "    #匹配文本中的中文\n",
    "    def match_chinese(self,line):\n",
    "        xx=u\"[\\u4e00-\\u9fa5]+\"\n",
    "        pattern = re.compile(xx)\n",
    "        line = ' '.join(pattern.findall(line))\n",
    "        return line\n",
    "\n",
    "\n",
    "    #对每行的简单修剪,可以根据具体的情况进行适当的处理\n",
    "    def simple_pruneline(self,line,file_read):\n",
    "        if line != \"\":\n",
    "            file_read =file_read+line\n",
    "        return file_read\n",
    "\n",
    "    #根据文本的情况适当的处理，文本的一些噪音等。自定义处理\n",
    "    def custom_pruneline(self,line,file_read):\n",
    "        if line.find(\"【 日  期 】\")!=-1:\n",
    "            line = \"\"\n",
    "        elif line.find(\"【 版  号 】\")!=-1:\n",
    "            line = \"\"\n",
    "        elif line.find(\"【 作  者 】\")!=-1:\n",
    "            line = \"\"\n",
    "        elif line.find(\"【 正  文 】\")!=-1:\n",
    "            line = \"\"\n",
    "        elif line.find(\"【 标  题 】\")!=-1:\n",
    "            line = \"\"\n",
    "        if line != \"\":\n",
    "            file_read += line\n",
    "        return file_read\n",
    "    \n",
    "\n",
    "    #对预处理后语料库进行分词，并持久化保存\n",
    "    def segment(self):\n",
    "        if (self.segment_path ==\"\" or self.pos_path ==\"\"):\n",
    "            print(\"segment_path or pos_path can not be empty.\")\n",
    "            return\n",
    "        #获取预处理后语料库的子目录\n",
    "        dir_list = os.listdir(self.pos_path)\n",
    "        #获得每个子目录的名字\n",
    "        for mydir in dir_list:\n",
    "            #得到分类子目录的路径\n",
    "            class_path =self.pos_path + mydir +\"/\"\n",
    "            #获得子目录下的所有文件\n",
    "            file_list =os.listdir(class_path)\n",
    "            for file_path in file_list:\n",
    "                #获得文件的全部路径，即全局路径\n",
    "                file_name =class_path +file_path\n",
    "                file_open = open(file_name,'rb')\n",
    "                file_read = file_open.read()\n",
    "                #进行结巴分词\n",
    "                seg_corpus = jieba.cut(file_read,cut_all=False)\n",
    "                seg_corpus = ' '.join(seg_corpus)\n",
    "                #分词后语料库的存储目录\n",
    "                seg_dir =self.segment_path +mydir +\"/\"\n",
    "                if not os.path.exists(seg_dir):\n",
    "                    os.makedirs(seg_dir)\n",
    "                file_write = open(seg_dir+file_path,'wb')\n",
    "                file_write.write(seg_corpus)\n",
    "                file_write.close()\n",
    "                file_open.close()\n",
    "        print(\"语料库的分词处理成功完成！\")\n",
    "        print(\"#######################################\")\n",
    "    \n",
    "\n",
    "    #打包分词后的语料库，持久化于data_set中,保存在wordbag_path下，命名为trainset_name\n",
    "    def train_set(self):\n",
    "        if (self.segment_path ==\"\" or self.wordbag_path == \"\" or self.trainset_name == \"\"):\n",
    "            print(\"segment_path(预处理后的文件路径) or wordbag_path(打包后存储的路径) or trainset_name(打包后存储的文件名) can not be empty.\")\n",
    "            return \n",
    "        #获取预处理后的文件路径\n",
    "        dir_list = os.listdir(self.segment_path)\n",
    "        self.data_set.target_name = dir_list\n",
    "        #获取每个目录下的所有文件\n",
    "        for mydir in dir_list:\n",
    "            class_path =self.segment_path + mydir +\"/\"\n",
    "            file_list  = os.listdir(class_path)\n",
    "            for file_path in file_list:\n",
    "                file_name = class_path + file_path\n",
    "                self.data_set.filenames.append(file_name)\n",
    "                #把文件分类标签附加到数据集中\n",
    "                self.data_set.label.append(self.data_set.target_name.index(mydir))\n",
    "                file_open =open(file_name,'rb')\n",
    "                seg_corpus =file_open.read()\n",
    "                self.data_set.content.append(seg_corpus)\n",
    "                file_open.close()\n",
    "        #将data_set对象持久化\n",
    "        if not os.path.exists(self.wordbag_path):\n",
    "            os.makedirs(self.wordbag_path)\n",
    "        file_obj = open(self.wordbag_path + self.trainset_name,'wb')\n",
    "        pickle.dump(self.data_set,file_obj)\n",
    "        file_obj.close()\n",
    "        print(\"分词后语料库打包到data_set中成功，并保存在trainset_name中。\")\n",
    "        print(\"#######################################\")\n",
    "\n",
    "\n",
    "    #计算语料的tf-idf值，并持久化于word_weight_bag，保存在wordbag_path目录下，命名为word_weight_bag_name。\n",
    "    def tfidf_bag(self):\n",
    "        if (self.wordbag_path == \"\" or self.word_weight_bag_name == \"\" or self.stopword_path ==\"\"):\n",
    "            print(\"wordbag_path(打包后存储的路径) or word_weight_bag_name(打包后存储的文件名) or stopword_path(停用词的路径) can not be empty. \")\n",
    "            return\n",
    "        #读取持久化后的data_set对象\n",
    "        file_obj = open(self.wordbag_path+self.trainset_name,'rb')\n",
    "        self.data_set = pickle.load(file_obj)\n",
    "        file_obj.close()\n",
    "        #设定word_weight_bag对象中的值\n",
    "        self.word_weight_bag.target_name =self.data_set.target_name\n",
    "        self.word_weight_bag.label =self.data_set.label\n",
    "        self.word_weight_bag.filenames =self.data_set.filenames\n",
    "        #构建语料库\n",
    "        corpus = self.data_set.content\n",
    "        stopword_list = self.getstopword(self.stopword_path)\n",
    "        #使用TfidfVectorizer初始化向量空间模型--创建词袋,具体的参数根据实际的情况设置\n",
    "        vectorizer = TfidfVectorizer(stop_words=stopword_list,sublinear_tf = True, max_features =10000)\n",
    "        #将文本转化为tfidf矩阵,是否需要.todense()函数？\n",
    "        self.word_weight_bag.tdm = vectorizer.fit_transform(corpus)\n",
    "        #保存词汇表\n",
    "        self.word_weight_bag.vocabulary =vectorizer.vocabulary_\n",
    "        #创建tfidf的持久化\n",
    "        if not os.path.exists(self.wordbag_path):\n",
    "            os.makedirs(self.wordbag_path)\n",
    "        file_obj1 = open(self.wordbag_path+self.word_weight_bag_name,'wb')\n",
    "        pickle.dump(self.word_weight_bag,file_obj1)\n",
    "        file_obj1.close()\n",
    "        print(\"tf-idf的持久化于word_weight_bag中，在wordbag_path目录中，命名为word_weight_bag_name，创建成功！\")\n",
    "        print(\"#######################################\")\n",
    "\n",
    "\n",
    "    \n",
    "    #统计语料库的词频矩阵，并持久化于vocabulary_count_bag，保存在wordbag_path目录下，命名为vocabulary_count_bag_name。\n",
    "    def voc_count_bag(self):\n",
    "        if (self.wordbag_path == \"\" or self.vocabulary_count_bag_name == \"\" or self.stopword_path ==\"\"):\n",
    "            print(\"wordbag_path(打包后存储的路径) or vocabulary_count_bag_name(打包后存储的文件名) or stopword_path(停用词的路径) can not be empty.\")\n",
    "            return \n",
    "        file_obj = open(self.wordbag_path+self.trainset_name,'rb')\n",
    "        self.data_set = pickle.load(file_obj)\n",
    "        file_obj.close()\n",
    "        #设定vocabulary_count_bag对象中的值\n",
    "        self.vocabulary_count_bag.target_name = self.data_set.target_name\n",
    "        self.vocabulary_count_bag.label =self.data_set.label\n",
    "        self.vocabulary_count_bag.filenames =self.data_set.filenames\n",
    "        corpus = self.data_set.content\n",
    "        stopword_list = self.getstopword(self.stopword_path)\n",
    "        #统计语料库的词频矩阵,参数可以根据实际的情况设置\n",
    "        vectorizer = CountVectorizer(stop_words=stopword_list, max_df=500, min_df=1,max_features=10000)\n",
    "        y = vectorizer.fit_transform(corpus)\n",
    "        self.vocabulary_count_bag.vcm = y\n",
    "        self.vocabulary_count_bag.vcm_sum = y.toarray().sum(axis=0)\n",
    "        self.vocabulary_count_bag.vocabulary = vectorizer.get_feature_names()\n",
    "        if not os.path.exists(self.wordbag_path):\n",
    "            os.makedirs(self.wordbag_path)\n",
    "        file_obj1 = open(self.wordbag_path+self.vocabulary_count_bag_name,'wb')\n",
    "        pickle.dump(self.vocabulary_count_bag,file_obj1)\n",
    "        file_obj1.close()\n",
    "        print(\"词频矩阵持久化于vocabulary_count_bag中，在wordbag_path目录中，命名为vocabulary_count_bag_name，创建成功！\")\n",
    "        print(\"#######################################\")\n",
    "\n",
    "    #导入停用词列表\n",
    "    def getstopword(self,stopword_path):\n",
    "        stop_file =open(stopword_path,'rb')\n",
    "        stop_content = stop_file.read()\n",
    "        stopword_list = stop_content.splitlines()\n",
    "        stop_file.close()\n",
    "        return stopword_list\n",
    "\n",
    "    #验证训练集data_set持久化结果\n",
    "    def verify_trainset(self):\n",
    "        file_obj = open(self.wordbag_path +self.trainset_name,'rb')\n",
    "        self.data_set = pickle.load(file_obj)\n",
    "        file_obj.close()\n",
    "        #输出数据集包含的所有类别\n",
    "        print(self.data_set.target_name)\n",
    "        #输出数据集包含的所有类别标签数\n",
    "        print(len(self.data_set.label))\n",
    "        #输出数据集包含的文件内容数\n",
    "        print(len(self.data_set.content))\n",
    "\n",
    "\n",
    "    #验证word_weight_bag持久化结果\n",
    "    def verify_word_weight_bag(self):\n",
    "        file_obj = open(self.wordbag_path +self.word_weight_bag_name,'rb')\n",
    "        self.word_weight_bag = pickle.load(file_obj)\n",
    "        file_obj.close()\n",
    "        #输出数据集包含的所有类别\n",
    "        print(self.word_weight_bag.target_name)\n",
    "        #输出数据集包含的所有类别标签数\n",
    "        print(len(self.word_weight_bag.label))\n",
    "        #输出数据集包含中矩阵的行列数\n",
    "        print(len(self.word_weight_bag.tdm.shape))\n",
    "        #输出词汇集的大小\n",
    "        print(len(self.word_weight_bag.vocabulary))\n",
    "\n",
    "    #验证vocabulary_count_bag持久化的结果\n",
    "    def verify_vocabulary_count_bag(self):\n",
    "        file_obj = open(self.wordbag_path +self.vocabulary_count_bag_name,'rb')\n",
    "        self.vocabulary_count_bag = pickle.load(file_obj)\n",
    "        file_obj.close()\n",
    "        #输出数据集包含的所有类别\n",
    "        print(self.vocabulary_count_bag.target_name)\n",
    "        #输出数据集包含的所有类别标签数\n",
    "        print(len(self.vocabulary_count_bag))\n",
    "        #输出数据集包含中矩阵的行列数\n",
    "        print(len(self.vocabulary_count_bag.vcm.shape))\n",
    "        #统计每个词汇的在语料库中出现的词频向量的长度\n",
    "        print(len(self.vocabulary_count_bag.vcm_sum))\n",
    "        #输出词汇集的大小\n",
    "        print(len(self.vocabulary_count_bag.vocabulary))\n",
    "\n",
    "    #进行tfidf的权值计算,参数：stopword_list停用词表；myvocabulary:导入的词典（自己的词典，可以是训练语料库的，也可以是自己的语料库）\n",
    "    def tfidf_value(self,test_data,myvocabulary):\n",
    "        vectorizer = TfidfVectorizer(vocabulary=myvocabulary)\n",
    "        return vectorizer.fit_transform(test_data)\n",
    "\n",
    "    #导出data_set\n",
    "    def load_trainset(self):\n",
    "        file_obj =open(self.wordbag_path + self.trainset_name,'rb')\n",
    "        self.data_set = pickle.load(file_obj)\n",
    "        file_obj.close()\n",
    "\n",
    "    #导出word_weight_bag\n",
    "    def load_word_weight_bag(self):\n",
    "        file_obj = open(self.wordbag_path +self.word_weight_bag_name,'rb')\n",
    "        self.word_weight_bag =pickle.load(file_obj)\n",
    "        file_obj.close()\n",
    "\n",
    "    #导出vocabulary_count_bag\n",
    "    def load_vocabulary_count_bag(self):\n",
    "        file_obj =open(self.wordbag_path+self.vocabulary_count_bag_name,'rb')\n",
    "        self.vocabulary_count_bag = pickle.load(file_obj)\n",
    "        file_obj.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tp = Textprocess()\n",
    "    tp.corpus_path = \"text_corpus_small/\"\n",
    "    tp.pos_path = \"text_corpus_pos/\"\n",
    "    #分词后的语料库路径\n",
    "    tp.segment_path =\"text_corpus_segment/\"\n",
    "    #词袋模型的语料库路径，也就是word_weight_bag，vocabulary_count_bag的存储路径\n",
    "    tp.wordbag_path =\"text_corpus_wordbag/\"\n",
    "    #停用词路径\n",
    "    tp.stopword_path =\"ch_stop_words.txt\"\n",
    "    #训练集和测试集的set存储的文件名\n",
    "    tp.trainset_name =\"trainset.dat\"\n",
    "    #词包存储的文件名\n",
    "    tp.word_weight_bag_name =\"word_weight_bag.dat\"\n",
    "    tp.vocabulary_count_bag_name =\"vocabulary_count_bag.dat\"\n",
    "    #文本的预处理\n",
    "    tp.preprocess()\n",
    "    #分词处理\n",
    "    tp.segment()\n",
    "    #持久化于data_set中\n",
    "    tp.train_set()\n",
    "    tp.tfidf_bag()\n",
    "    tp.voc_count_bag()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opposite-remains",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prescribed-controversy",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'base' from 'sklearn.datasets' (/usr/local/lib/python3.7/site-packages/sklearn/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-42b20217b67b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'base' from 'sklearn.datasets' (/usr/local/lib/python3.7/site-packages/sklearn/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fsfihidsad'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"fsfihi{}\".format('dsad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-occasions",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
